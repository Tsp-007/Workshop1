{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb2f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "779218e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1Ô∏è‚É£: Split CSV ‡πÄ‡∏õ‡πá‡∏ô train/test\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.to_csv(\"train_split.csv\", index=False)\n",
    "test_df.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "# Step 2Ô∏è‚É£: Prepare encoders\n",
    "color_encoder = LabelEncoder()\n",
    "type_encoder = LabelEncoder()\n",
    "train_df['color'] = color_encoder.fit_transform(train_df['color'])\n",
    "train_df['type'] = type_encoder.fit_transform(train_df['type'])\n",
    "train_df.to_csv(\"train_split_encoded.csv\", index=False)\n",
    "\n",
    "test_df['color'] = color_encoder.transform(test_df['color'])\n",
    "test_df['type'] = type_encoder.transform(test_df['type'])\n",
    "test_df.to_csv(\"test_split_encoded.csv\", index=False)\n",
    "\n",
    "# Step 3Ô∏è‚É£: Transforms\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_mel_raw = transforms.Compose([\n",
    "    transforms.Resize((128, 431)),\n",
    "    transforms.ToTensor()  # Grayscale ‚Üí Tensor [1,H,W]\n",
    "])\n",
    "\n",
    "def transform_mel(mel_tensor, target_shape=(128, 431)):\n",
    "    mel_tensor = mel_tensor.unsqueeze(0)  # (1, 1, H, W)\n",
    "    mel_tensor = F.interpolate(\n",
    "        mel_tensor, size=target_shape, mode='bilinear', align_corners=False\n",
    "    ).squeeze(0)  # (1, target_H, target_W)\n",
    "    mel_tensor = (mel_tensor - mel_tensor.min()) / (mel_tensor.max() - mel_tensor.min() + 1e-5)\n",
    "    return mel_tensor\n",
    "\n",
    "# Step 4Ô∏è‚É£: Dataset Class\n",
    "class FashionMultiModalDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, mel_dir):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.mel_dir = mel_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_name = row['image_name']\n",
    "\n",
    "        # Load Image\n",
    "        image_path = f\"{self.image_dir}/{file_name}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform_image(image)\n",
    "\n",
    "        # Load Mel-Spectrogram\n",
    "        mel_key = file_name.replace('.jpg', '')  # Remove .jpg if needed\n",
    "        mel_path = f\"{self.mel_dir}/{mel_key}.png\"\n",
    "        mel_img = Image.open(mel_path).convert(\"L\")\n",
    "        mel_tensor = transform_mel_raw(mel_img)\n",
    "        mel_tensor = transform_mel(mel_tensor)\n",
    "\n",
    "        # Labels\n",
    "        label_color = torch.tensor(row['color'], dtype=torch.long)\n",
    "        label_type  = torch.tensor(row['type'], dtype=torch.long)\n",
    "\n",
    "        label_condition = torch.tensor(int(row['condition']) - 1, dtype=torch.long)  # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô class index 0‚Äì4\n",
    "        label_pilling   = torch.tensor(int(row['pilling']) - 1, dtype=torch.long)    # ‚úÖ ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "        label_smell     = torch.tensor(int(row['smell']), dtype=torch.float32)       # ‚úÖ ‡πÉ‡∏ä‡πâ BCEWithLogitsLoss\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"mel\": mel_tensor,\n",
    "            \"color\": label_color,\n",
    "            \"type\": label_type,\n",
    "            \"condition\": label_condition,\n",
    "            \"pilling\": label_pilling,\n",
    "            \"smell\": label_smell\n",
    "        }\n",
    "\n",
    "# Step 5Ô∏è‚É£: DataLoaders\n",
    "train_dataset = FashionMultiModalDataset(\"train_split_encoded.csv\", \"./image\", \"./mel\")\n",
    "test_dataset = FashionMultiModalDataset(\"test_split_encoded.csv\", \"./image\", \"./mel\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d671673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_colors, num_types):\n",
    "        super().__init__()\n",
    "\n",
    "        # üñºÔ∏è Image Encoder (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B,3,224,224] ‚Üí [B,32,224,224]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B,32,112,112]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # [B,64,112,112]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B,64,56,56]\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),                 # [B,128,1,1]\n",
    "            nn.Flatten(),                                # [B,128]\n",
    "        )\n",
    "\n",
    "        # üîä Audio Encoder (mel: [B,1,128,431])\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B,16,64,215]\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # [B,32,32,107]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),                 # [B,64,1,1]\n",
    "            nn.Flatten(),                                # [B,64]\n",
    "        )\n",
    "\n",
    "        # üéØ Classifiers\n",
    "        self.classifier_color = nn.Linear(128, num_colors)\n",
    "        self.classifier_type = nn.Linear(128, num_types)\n",
    "        self.classifier_condition = nn.Linear(64, 5)\n",
    "        self.classifier_pilling   = nn.Linear(64, 5)\n",
    "        self.classifier_smell = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, image, mel):\n",
    "        img_feat = self.image_encoder(image)    # [B,128]\n",
    "        mel_feat = self.audio_encoder(mel)      # [B,64]\n",
    "\n",
    "        pred_color = self.classifier_color(img_feat)\n",
    "        pred_type = self.classifier_type(img_feat)\n",
    "\n",
    "        pred_condition = self.classifier_condition(mel_feat)\n",
    "        pred_pilling = self.classifier_pilling(mel_feat)\n",
    "        pred_smell = self.classifier_smell(mel_feat)\n",
    "\n",
    "        return pred_color, pred_type, pred_condition, pred_pilling, pred_smell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92eeafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=10, lr=1e-3, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn_ce = nn.CrossEntropyLoss()\n",
    "    loss_fn_bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        acc_color, acc_type = 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            image = batch['image'].to(device)\n",
    "            mel = batch['mel'].to(device)\n",
    "\n",
    "            # üéØ Targets\n",
    "            color = batch['color'].to(device)\n",
    "            type_ = batch['type'].to(device)\n",
    "            condition = batch['condition'].to(device)       # dtype: long ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
    "            pilling   = batch['pilling'].to(device)\n",
    "            smell     = batch['smell'].to(device).float()  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BCE\n",
    "\n",
    "            # üöÄ Forward\n",
    "            pred_color, pred_type, pred_condition, pred_pilling, pred_smell = model(image, mel)\n",
    "\n",
    "            # üéØ Loss ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ head\n",
    "            loss_color = loss_fn_ce(pred_color, color)\n",
    "            loss_type  = loss_fn_ce(pred_type, type_)\n",
    "            loss_condition = loss_fn_ce(pred_condition, condition)\n",
    "            loss_pilling   = loss_fn_ce(pred_pilling, pilling)\n",
    "            loss_smell     = loss_fn_bce(pred_smell.squeeze(), smell)\n",
    "\n",
    "            # ‚úÖ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "            loss = loss_color + loss_type + loss_condition + loss_pilling + loss_smell\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            acc_color += (pred_color.argmax(1) == color).float().mean().item()\n",
    "            acc_type  += (pred_type.argmax(1) == type_).float().mean().item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc_color = acc_color / len(train_loader)\n",
    "        avg_acc_type  = acc_type / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Color Acc: {avg_acc_color:.3f} | Type Acc: {avg_acc_type:.3f}\")\n",
    "\n",
    "    print(\"‚úÖ Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43aec0c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üîß ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™\n",
    "num_colors = len(color_encoder.classes_)\n",
    "num_types = len(type_encoder.classes_)\n",
    "\n",
    "# üéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "model = MultiModalModel(num_colors=num_colors, num_types=num_types)\n",
    "\n",
    "# üöÄ ‡πÄ‡∏ó‡∏£‡∏ô!\n",
    "trained_model = train_model(model, train_loader, test_loader, num_epochs=1, lr=1e-3, device='cpu')\n",
    "\n",
    "# üíæ ‡πÄ‡∏ã‡∏ü‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "torch.save(trained_model.state_dict(), \"multimodal_model.pth\")\n",
    "print(\"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà multimodal_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c6e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
